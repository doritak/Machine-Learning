{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2 Curso 4 Parte 3: Aplicaciones en Ciencias de Datos e Inteligencia Artificial\n",
    "\n",
    "### 3 Deploy (opcional)\n",
    "\n",
    "**Autor: Dora Novoa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Incluya los cuatro modelos en una clase, de modo que se puedan ajustar, visualizar y predecir con las\n",
    "tres variantes. Considere los parámetros que requiere cada opción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class regression():\n",
    "    def __init__(self, tipo_regresion, x, y, grado=None, alpha=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tipo_regresion = tipo_regresion\n",
    "        self.grado = grado\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def escalar_datos(self, X):\n",
    "        # estandarizar los datos\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "    \n",
    "    def training_test_split(self, X, test_s=0.2, ran_state=13):\n",
    "        # divide los datos 80%/20% de forma aleatoria donde el 20% será para pruebas\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, self.y, test_size=test_s, random_state=ran_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def seleccionar_regresion(self):\n",
    "        if self.x.shape[1] == 1:\n",
    "            X = self.x.reshape(-1, 1)\n",
    "            X_std = self.escalar_datos(X)\n",
    "        else:\n",
    "            X_std = self.escalar_datos(self.x)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = self.training_test_split(X_std)\n",
    "        \n",
    "        match self.tipo_regresion:\n",
    "            case \"lineal\":\n",
    "                modelo_lineal = LinearRegression()\n",
    "                modelo_lineal.fit(X_std, self.y)\n",
    "                y_pred_lin = modelo_lineal.predict(X_std)\n",
    "\n",
    "                # graficamos\n",
    "                titulo = self.imprimir_formula(modelo_lineal.intercept_, modelo_lineal.coef_, self.tipo_regresion)\n",
    "                self.graficar_regresion(X_std, y_pred_lin, titulo)\n",
    "                \n",
    "                return modelo_lineal\n",
    "            \n",
    "            case \"polinomial\":\n",
    "                modelo_poly = PolynomialFeatures(self.grado)\n",
    "                X_poly = modelo_poly.fit_transform(X_std)\n",
    "                modelo = LinearRegression()\n",
    "                modelo.fit(X_poly, self.y)\n",
    "                \n",
    "                x_linea = np.linspace(X_std.min(), X_std.max(), 100).reshape(-1, 1) # hago la linea con los valores min. y max. de X\n",
    "                y_linea = modelo.predict(modelo_poly.transform(x_linea))\n",
    "                \n",
    "                # graficamos\n",
    "                titulo = self.imprimir_formula(modelo.intercept_, modelo.coef_, self.tipo_regresion)\n",
    "                self.graficar_regresion(x_linea, y_linea, titulo)\n",
    "                \n",
    "                return modelo\n",
    "            \n",
    "            case \"lasso\":\n",
    "                # if X_std.shape[1] == 1:\n",
    "                    # como la matriz tiene sólo una columna, debo transformarla a polinomial elevando a la potencia del grado\n",
    "                poly = PolynomialFeatures(self.grado)\n",
    "                X_train_poly = poly.fit_transform(X_train)\n",
    "                X_test_poly = poly.fit_transform(X_test)\n",
    "                                    \n",
    "                lasso = Lasso(alpha=self.alpha).fit(X_train_poly, y_train)\n",
    "                y_pred_l = lasso.predict(X_test_poly)\n",
    "                print('Error Cuadrático Medio Lasso:', mean_squared_error(y_test, y_pred_l))\n",
    "                \n",
    "                # buscar mi mejor alpha con una validación cruzada de 5 iteraciones\n",
    "                CV = 5\n",
    "                best_alpha = self.obtener_best_alpha(lasso, CV, X_train_poly, y_train)\n",
    "                print('Mejor alpha:', best_alpha)\n",
    "                \n",
    "                # ajustar el modelo con el mejor alpha\n",
    "                lasso_best = Lasso(alpha=best_alpha).fit(X_train_poly, y_train)\n",
    "                y_pred_l_best = lasso_best.predict(X_test_poly)\n",
    "                print('Error Cuadrático Medio best Lasso:', mean_squared_error(y_test, y_pred_l_best))\n",
    "                \n",
    "                # usamos el linspace para generar una uniforme línea de ajuste en X\n",
    "\n",
    "                x_linea = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)\n",
    " \n",
    "                x_linea_poly = poly.fit_transform(x_linea)\n",
    "                y_pred_l_poly = lasso_best.predict(x_linea_poly)\n",
    "                \n",
    "                # graficamos\n",
    "                titulo = self.imprimir_formula(lasso_best.intercept_, lasso_best.coef_, self.tipo_regresion)\n",
    "                self.graficar_regresion(x_linea, y_pred_l_poly, titulo)\n",
    "                \n",
    "                return lasso_best\n",
    "                \n",
    "                # else:\n",
    "                    # lasso = Lasso(alpha=self.alpha).fit(X_train, y_train)\n",
    "                    # y_pred_l = lasso.predict(X_test)\n",
    "                    # print('Error Cuadrático Medio Lasso:', mean_squared_error(y_test, y_pred_l))\n",
    "                    \n",
    "                    # # buscar mi mejor alpha con una validación cruzada de 5 iteraciones\n",
    "                    # CV = 5\n",
    "                    # best_alpha = self.obtener_best_alpha(lasso, CV, X_train, y_train)\n",
    "                    # print('Mejor alpha:', best_alpha)\n",
    "                    \n",
    "                    # # ajustar el modelo con el mejor alpha\n",
    "                    # lasso_best = Lasso(alpha=best_alpha).fit(X_train, y_train)\n",
    "                    # y_pred_l_best = lasso_best.predict(X_test)\n",
    "                    # print('Error Cuadrático Medio best Lasso:', mean_squared_error(y_test, y_pred_l_best))\n",
    "                    # # usamos el linspace para generar una uniforme línea de ajuste en X\n",
    "                    # x_linea = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)\n",
    "                    \n",
    "                    \n",
    "                    # # graficamos\n",
    "                    # titulo = self.imprimir_formula(lasso_best.intercept_, lasso_best.coef_, self.tipo_regresion)\n",
    "                    # self.graficar_regresion(x_linea, y_pred_l_best, titulo)\n",
    "                    \n",
    "                    # return lasso_best\n",
    "                    \n",
    "\n",
    "            case \"ridge\":\n",
    "                if X_std.shape[1] == 1:\n",
    "                    # como la matriz tiene sólo una columna, debo transformarla a polinomial elevando a la potencia del grado\n",
    "                    poly = PolynomialFeatures(self.grado)\n",
    "                    X_train_poly = poly.fit_transform(X_train)\n",
    "                    X_test_poly = poly.fit_transform(X_test)\n",
    "                    \n",
    "                    ridge = Ridge(alpha=self.alpha).fit(X_train_poly, y_train)\n",
    "                    y_pred_r = ridge.predict(X_test_poly)\n",
    "                    print('Error Cuadrático Medio Ridge:', mean_squared_error(y_test, y_pred_r))\n",
    "                    \n",
    "                    # buscar mi mejor alpha con una validación cruzada de 5 iteraciones\n",
    "                    CV = 5\n",
    "                    best_alpha = self.obtener_best_alpha(ridge, CV, X_train_poly, y_train)\n",
    "                    print('Mejor alpha:', best_alpha)\n",
    "                    \n",
    "                    # ajustar el modelo con el mejor alpha\n",
    "                    ridge_best = Ridge(alpha=best_alpha).fit(X_train_poly, y_train)\n",
    "                    y_pred_r_best = ridge_best.predict(X_test_poly)\n",
    "                    print('Error Cuadrático Medio best Ridge:', mean_squared_error(y_test, y_pred_r_best))\n",
    "                    \n",
    "                    # usamos el linspace para generar una uniforme línea de ajuste en X\n",
    "                    x_linea = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "                    x_linea_poly = poly.fit_transform(x_linea)\n",
    "                    y_pred_r_poly = ridge_best.predict(x_linea_poly)\n",
    "                    \n",
    "                    # graficamos\n",
    "                    titulo = self.imprimir_formula(ridge_best.intercept_, ridge_best.coef_, self.tipo_regresion)\n",
    "                    self.graficar_regresion(x_linea, y_pred_r_poly, titulo)\n",
    "                    \n",
    "                    return ridge_best\n",
    "                \n",
    "                else:\n",
    "                    ridge = Ridge(alpha=self.alpha).fit(X_train, y_train)\n",
    "                    y_pred_r = ridge.predict(X_test)\n",
    "                    print('Error Cuadrático Medio Ridge:', mean_squared_error(y_test, y_pred_r))\n",
    "                    \n",
    "                    # buscar mi mejor alpha con una validación cruzada de 5 iteraciones\n",
    "                    CV = 5\n",
    "                    best_alpha = self.obtener_best_alpha(ridge, CV, X_train, y_train)\n",
    "                    print('Mejor alpha:', best_alpha)\n",
    "                    \n",
    "                    # ajustar el modelo con el mejor alpha\n",
    "                    ridge_best = Ridge(alpha=best_alpha).fit(X_train, y_train)\n",
    "                    y_pred_r_best = lasso_best.predict(X_test)\n",
    "                    print('Error Cuadrático Medio best Ridge:', mean_squared_error(y_test, y_pred_r_best))\n",
    "\n",
    "                    # graficamos\n",
    "                    titulo = self.imprimir_formula(ridge_best.intercept_, ridge_best.coef_, self.tipo_regresion)\n",
    "                    self.graficar_regresion(X_test, y_pred_r_best, titulo)\n",
    "                    \n",
    "                    return ridge_best\n",
    "    \n",
    "    # función para obtener el mejor alpha con validación cruzada\n",
    "    def obtener_best_alpha(self, modelo, val_iteraciones, X_train, y_train):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "       \n",
    "        param_grid = {'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]} # valores de alpha a probar\n",
    "        modelo_CV = GridSearchCV(modelo, param_grid, cv=val_iteraciones, n_jobs=-1) # cross validation de val_iteraciones\n",
    "        modelo_CV.fit(X_train, y_train)\n",
    "        best_alpha = modelo_CV.best_params_['alpha']\n",
    "        return best_alpha\n",
    "    \n",
    "    # función para imprimir la fórmula de la regresión\n",
    "    def imprimir_formula (self, intercepto, coefs, nombre_modelo):\n",
    "        if nombre_modelo == \"lineal\":\n",
    "            formula = f\"y = {intercepto.round(2)} + {coefs[0].round(2)}*x\"\n",
    "        else: \n",
    "            formula = f\"y = {intercepto.round(2)} \"\n",
    "            # para cada uno de los coefs, es decir los thetas o x^n \n",
    "            for i, coef in enumerate(coefs):\n",
    "                if abs(coef) > 0.00000001: # si el coeficiente es muy pequeño no lo toma en cuenta\n",
    "                    formula += f\"+ {coef.round(3)}x^{i} \"    \n",
    "        return formula\n",
    "    \n",
    "    # función para graficar la regresión            \n",
    "    def graficar_regresion(self, x_line, y_line, titulo=None):\n",
    "        X_std = self.escalar_datos(self.x)\n",
    "        plt.plot(x_line, y_line, color='red') # graficar la recta de ajuste\n",
    "        plt.scatter(X_std, self.y, color='blue', alpha=0.4) # graficar los datos \n",
    "        if titulo:\n",
    "            plt.title(\"Regresión \" + str(self.tipo_regresion).capitalize() + \"\\n\" + titulo)\n",
    "        else:\n",
    "            plt.title(\"Regresión \\n\" + self.tipo_regresion)\n",
    "        plt.xlabel(\"Eje x\")\n",
    "        plt.ylabel(\"Eje y\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Genere una función para guardar el modelo. Para esto puede guardar los coeficientes en un archivo\n",
    "o puede usar una librería como Pickle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsume\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     40\u001b[0m regresion \u001b[38;5;241m=\u001b[39m regression(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m modelo_result \u001b[38;5;241m=\u001b[39m \u001b[43mregresion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseleccionar_regresion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m df_r \u001b[38;5;241m=\u001b[39m guardar_modelo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m\"\u001b[39m, modelo_result)\n\u001b[0;32m     43\u001b[0m df_r\n",
      "Cell \u001b[1;32mIn[55], line 29\u001b[0m, in \u001b[0;36mregression.seleccionar_regresion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseleccionar_regresion\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     30\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m         X_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mescalar_datos(X)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def guardar_modelo(nombre_reg, modelo):\n",
    "    \n",
    "    # los coeficientes del modelo y el intercepto los guardo primero en una lista\n",
    "    datos = [modelo.intercept_]\n",
    "    datos.extend(modelo.coef_)\n",
    "    \n",
    "    # guardar los índices de los coeficientes en un dataframe\n",
    "    coefs = []\n",
    "    indices = []\n",
    "    for i, coef in enumerate(datos):\n",
    "        if abs(coef) > 0.0000000001: # si el coeficiente es muy pequeño no lo toma en cuenta\n",
    "            coefs.append(coef)\n",
    "            if i == 0: indices.append(\"Intercepto\") # agrega el intercepto\n",
    "            else: indices.append(f\"x^{i}\")\n",
    "    \n",
    "    # crear el dataframe\n",
    "    df = pd.DataFrame(coefs, index=indices , columns=['Coeficientes'])\n",
    "    \n",
    "    # los dejaré en una subcarpeta llamada datos_exportados, si no existe la creará\n",
    "    carpeta = \"datos_exportados\"\n",
    "    if not os.path.exists(carpeta):\n",
    "        os.makedirs(carpeta)\n",
    "        \n",
    "    # guardar en un archivo csv, los dejo en la carpeta datos_exportados  \n",
    "    ruta_archivo = os.path.join(carpeta, f\"coef_Regresion_{nombre_reg}.csv\")\n",
    "    df.to_csv(ruta_archivo)\n",
    "    print(f\"Coeficientes guardados en {ruta_archivo}\")\n",
    "    \n",
    "    return df\n",
    "## Aqui podemos cambiar a la regresión que queramos ejercutar, como lineal, polinomial, lasso, ridge\n",
    "df = pd.read_csv(\"measurements.csv\", encoding=\"latin-1\", sep=\",\")\n",
    "\n",
    "x = df[\"distance\"].str.replace(',', '.').astype(float).values\n",
    "y = df[\"consume\"].str.replace(',', '.').astype(float).values\n",
    "\n",
    "\n",
    "regresion = regression(\"lasso\", x, y, 6, 0.001)\n",
    "modelo_result = regresion.seleccionar_regresion()\n",
    "df_r = guardar_modelo(\"lasso\", modelo_result)\n",
    "df_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Genere una función para cargar y predecir usando el modelo cargado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_predecir(ruta, var_ind:list, var_dep, nombre_modelo):\n",
    "    # var_ind es un lista con el nombre de las columnas de variables independientes\n",
    "    # cargar los datos\n",
    "    df = pd.read_csv({ruta}, encoding=\"latin-1\", sep=\",\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cree un notebook que le permita usar la clase creada. Implemente tres ejemplos de uso.\n",
    "* ¿Cómo podría testear la clase implementada?, proponga método para testear que todo está\n",
    "funcionando bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cuadrático Medio Lasso: 1.4480751820909503\n",
      "Mejor alpha: 0.001\n",
      "Error Cuadrático Medio best Lasso: 1.4480751820909503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e-02, tolerance: 2.486e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e-02, tolerance: 2.486e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e-02, tolerance: 2.486e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100, 1) and (78,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m df[var_dep]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     17\u001b[0m regresion \u001b[38;5;241m=\u001b[39m regression(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m modelo_result \u001b[38;5;241m=\u001b[39m \u001b[43mregresion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseleccionar_regresion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 94\u001b[0m, in \u001b[0;36mregression.seleccionar_regresion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# graficamos\u001b[39;00m\n\u001b[0;32m     93\u001b[0m titulo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimprimir_formula(lasso_best\u001b[38;5;241m.\u001b[39mintercept_, lasso_best\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtipo_regresion)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraficar_regresion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_linea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_l_poly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitulo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lasso_best\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# lasso = Lasso(alpha=self.alpha).fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# y_pred_l = lasso.predict(X_test)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# return lasso_best\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 201\u001b[0m, in \u001b[0;36mregression.graficar_regresion\u001b[1;34m(self, x_line, y_line, titulo)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgraficar_regresion\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_line, y_line, titulo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    200\u001b[0m     X_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mescalar_datos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m--> 201\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# graficar la recta de ajuste\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(X_std, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m) \u001b[38;5;66;03m# graficar los datos \u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m titulo:\n",
      "File \u001b[1;32mc:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doran\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100, 1) and (78,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"measurements.csv\", encoding=\"latin-1\", sep=\",\")\n",
    "df.dtypes\n",
    "var_ind = [\"distance\",\"speed\",\"temp_inside\",\"temp_outside\"]\n",
    "var_dep = \"consume\" \n",
    "df_use = pd.DataFrame()\n",
    "\n",
    "for nombre_col in df.columns.values:\n",
    "    if nombre_col in var_ind:\n",
    "        if df[nombre_col].dtype == 'object':\n",
    "            df_use[nombre_col] = df[nombre_col].str.replace(',', '.').fillna(0).astype(float)\n",
    "        else:  # Si ya es numérica (int o float), copiar directamente\n",
    "            df_use[nombre_col] = df[nombre_col]\n",
    "    \n",
    "x = df_use\n",
    "y = df[var_dep].str.replace(',', '.').astype(float).values\n",
    "\n",
    "regresion = regression(\"lasso\", x, y, 6, 0.001)\n",
    "modelo_result = regresion.seleccionar_regresion()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
